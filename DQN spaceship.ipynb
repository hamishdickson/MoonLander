{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"LunarLander-v2\"\n",
    "\n",
    "GAMMA = 0.999\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "MEMORY_SIZE = 25000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EXPLORATION_MAX = 0.3\n",
    "EXPLORATION_MIN = 0.2\n",
    "EXPLORATION_DECAY = 0.9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvModel(nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(observation_space, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc_out = nn.Linear(64, action_space)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn():\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.model = EnvModel(observation_space, action_space)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.train_losses = []\n",
    "        self.epsilon = EXPLORATION_MAX\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)    \n",
    "        \n",
    "        \n",
    "    def fit(self, state, q_values):\n",
    "        q_values = torch.FloatTensor(q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        log_ps = self.model(state)\n",
    "        loss = self.criterion(log_ps, q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "    def predict(self, state):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model(state)\n",
    "        \n",
    "        self.model.train()\n",
    "        return prediction.numpy()\n",
    "    \n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        \n",
    "    def get_action(self, state):  \n",
    "        random_action = random.random() <= self.epsilon\n",
    "        q_values = self.predict(state)\n",
    "        return random.randint(0, self.action_space - 1) if random_action else np.argmax(q_values)\n",
    "    \n",
    "    \n",
    "    def experience_replay(self):\n",
    "        batch = random.sample(self.memory, min(BATCH_SIZE, len(self.memory)))\n",
    "\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            state_reward = reward if terminal else (reward + GAMMA * np.max(self.predict(state_next)))\n",
    "            q_values_predicted = self.predict(state)\n",
    "            q_values_predicted[action] = state_reward\n",
    "            self.fit(state, q_values_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 4\n"
     ]
    }
   ],
   "source": [
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "print(observation_space, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "exploration_rate = EXPLORATION_MAX\n",
    "\n",
    "scores = []\n",
    "final_rewards = []\n",
    "\n",
    "dqn = Dqn(observation_space, action_space)\n",
    "\n",
    "landed_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, epsilon: 0.29698510012725077, steps: 102, reward: -100\n",
      "Epoch: 2, epsilon: 0.2926511351436448, steps: 148, reward: -100\n",
      "Epoch: 3, epsilon: 0.28997095840733844, steps: 93, reward: -100\n",
      "Epoch: 4, epsilon: 0.2869707386126297, steps: 105, reward: -100\n",
      "Epoch: 5, epsilon: 0.2843994615030759, steps: 91, reward: -100\n",
      "Epoch: 6, epsilon: 0.2815413418354238, steps: 102, reward: -100\n",
      "Epoch: 7, epsilon: 0.2741232888122292, steps: 268, reward: -100\n",
      "Epoch: 8, epsilon: 0.27125991015489165, steps: 106, reward: -100\n",
      "Epoch: 9, epsilon: 0.26880252018416595, steps: 92, reward: -100\n",
      "Epoch: 10, epsilon: 0.2665272685172853, steps: 86, reward: -100\n",
      "Epoch: 11, epsilon: 0.26342691688148634, steps: 118, reward: -100\n",
      "Epoch: 12, epsilon: 0.2599723590918361, steps: 133, reward: -100\n",
      "Epoch: 13, epsilon: 0.2578749839499712, steps: 82, reward: -100\n",
      "Epoch: 14, epsilon: 0.25500275502368414, steps: 113, reward: -100\n",
      "Epoch: 15, epsilon: 0.2519860563597597, steps: 120, reward: -100\n",
      "Epoch: 16, epsilon: 0.24920433913921267, steps: 112, reward: -100\n",
      "Epoch: 17, epsilon: 0.24635476322564684, steps: 116, reward: -100\n",
      "Epoch: 18, epsilon: 0.2434160268126634, steps: 121, reward: -100\n",
      "Epoch: 19, epsilon: 0.24027194208922759, steps: 131, reward: -100\n",
      "Epoch: 20, epsilon: 0.23795245596998665, steps: 98, reward: -100\n",
      "Epoch: 21, epsilon: 0.23567892910492189, steps: 97, reward: -100\n",
      "Epoch: 22, epsilon: 0.2310119426075417, steps: 201, reward: -100\n",
      "Epoch: 23, epsilon: 0.22704961087830489, steps: 174, reward: -100\n",
      "Epoch: 24, epsilon: 0.22393769227647448, steps: 139, reward: -100\n",
      "Epoch: 25, epsilon: 0.21992068280079521, steps: 182, reward: -100\n",
      "Epoch: 26, epsilon: 0.21612697421573168, steps: 175, reward: -100\n",
      "Epoch: 27, epsilon: 0.20967622672639782, steps: 304, reward: -100\n",
      "Epoch: 28, epsilon: 0.2046422532114821, steps: 244, reward: -100\n",
      "Epoch: 29, epsilon: 0.20101156693055053, steps: 180, reward: -100\n",
      "Epoch: 30, epsilon: 0.2, steps: 110, reward: -100\n",
      "Epoch: 31, epsilon: 0.2, steps: 591, reward: -100\n",
      "Epoch: 32, epsilon: 0.2, steps: 412, reward: -100\n",
      "Epoch: 33, epsilon: 0.2, steps: 121, reward: -100\n",
      "Epoch: 34, epsilon: 0.2, steps: 152, reward: -100\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for run in range(epochs):\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        step += 1\n",
    "        action = dqn.get_action(state)\n",
    "        \n",
    "        state_next, reward, terminal, info = env.step(action)\n",
    "        \n",
    "        if terminal and reward >= 50:\n",
    "            print(\"landed: \" + str(reward))\n",
    "            landed_count += 1\n",
    "        \n",
    "        dqn.remember(state, action, reward, state_next, terminal)\n",
    "        \n",
    "        state = state_next\n",
    "        \n",
    "        if terminal:\n",
    "            print(\"Epoch: \" + str(run + 1) + \", epsilon: \" + str(dqn.epsilon) + \", steps: \" + str(step) + \", reward: \" + str(reward))\n",
    "            scores.append(step)\n",
    "            final_rewards.append(reward)\n",
    "            break\n",
    "            \n",
    "        dqn.experience_replay()\n",
    "        \n",
    "        dqn.epsilon *= EXPLORATION_DECAY\n",
    "        dqn.epsilon = max(EXPLORATION_MIN, dqn.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x122a802b0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(final_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this breaks the kernel\n",
    "\n",
    "\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "video_env = Monitor(env, './test_video', force=True)\n",
    "\n",
    "state = video_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "#     video_env.render()\n",
    "#     video_env.render(close=True)\n",
    "    action = np.argmax(dqn.predict(state))\n",
    "    state, reward, done, info = video_env.step(action)\n",
    "#     print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
